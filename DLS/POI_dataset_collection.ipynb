{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-17T09:34:12.422066Z",
     "start_time": "2025-06-17T09:34:09.674748Z"
    }
   },
   "source": [
    "!pip install requests pandas\n",
    "!pip install tqdm"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T11:14:04.062764Z",
     "start_time": "2025-06-17T09:34:12.434651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "OVERPASS_URL = \"http://overpass-api.de/api/interpreter\"\n",
    "WIKIPEDIA_API_URL = \"https://ru.wikipedia.org/w/api.php\"\n",
    "WIKIDATA_API_URL = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "all_processed_pois = []\n",
    "\n",
    "def get_wikipedia_summary(title):\n",
    "    \"\"\"Получает краткое описание из Википедии по названию статьи.\"\"\"\n",
    "    try:\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"exintro\": True,\n",
    "            \"explaintext\": True,\n",
    "            \"redirects\": 1\n",
    "        }\n",
    "        response = requests.get(WIKIPEDIA_API_URL, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        page_id = next(iter(data['query']['pages']))\n",
    "        if page_id != '-1':\n",
    "            return data['query']['pages'][page_id].get('extract', '')\n",
    "        return \"\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Таймаут при запросе к Википедии для {title}\")\n",
    "        return \"\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def get_wikidata_description(qid):\n",
    "    try:\n",
    "        params = {\n",
    "            \"action\": \"wbgetentities\",\n",
    "            \"format\": \"json\",\n",
    "            \"ids\": qid,\n",
    "            \"props\": \"descriptions\",\n",
    "            \"languages\": \"ru|en\"\n",
    "        }\n",
    "        response = requests.get(WIKIDATA_API_URL, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        entity = data['entities'].get(qid)\n",
    "        if entity and 'descriptions' in entity:\n",
    "            if 'ru' in entity['descriptions']:\n",
    "                return entity['descriptions']['ru']['value']\n",
    "            elif 'en' in entity['descriptions']:\n",
    "                return entity['descriptions']['en']['value']\n",
    "        return \"\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Таймаут при запросе к Wikidata для {qid}\")\n",
    "        return \"\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def create_text_description(tags):\n",
    "    description_parts = []\n",
    "\n",
    "    if 'name' in tags:\n",
    "        description_parts.append(tags['name'])\n",
    "\n",
    "    tag_translations = {\n",
    "        'amenity': 'Тип',\n",
    "        'shop': 'Магазин',\n",
    "        'tourism': 'Туризм',\n",
    "        'leisure': 'Досуг',\n",
    "        'historic': 'Исторический объект',\n",
    "        'cuisine': 'Кухня'\n",
    "    }\n",
    "\n",
    "    for key, name in tag_translations.items():\n",
    "        if key in tags:\n",
    "            description_parts.append(f\"{name}: {tags[key].replace('_', ' ')}\")\n",
    "\n",
    "    if 'opening_hours' in tags:\n",
    "        description_parts.append(f\"Часы работы: {tags['opening_hours']}\")\n",
    "    if 'wheelchair' in tags and tags['wheelchair'] == 'yes':\n",
    "        description_parts.append(\"Доступно для инвалидных колясок\")\n",
    "    if 'internet_access' in tags and tags['internet_access'] == 'yes':\n",
    "        description_parts.append(\"Есть доступ в интернет\")\n",
    "    if 'phone' in tags:\n",
    "        description_parts.append(f\"Телефон: {tags['phone']}\")\n",
    "    if 'website' in tags:\n",
    "        description_parts.append(f\"Вебсайт: {tags['website']}\")\n",
    "\n",
    "    if 'description' in tags:\n",
    "        description_parts.append(f\"Описание: {tags['description']}\")\n",
    "\n",
    "    extra_description = \"\"\n",
    "    if 'wikipedia' in tags:\n",
    "        parts = tags['wikipedia'].split(':')\n",
    "        if len(parts) >= 2:\n",
    "            lang = parts[0]\n",
    "            title = parts[-1]\n",
    "            if lang == 'ru':\n",
    "                wiki_summary = get_wikipedia_summary(title)\n",
    "                if wiki_summary:\n",
    "                    extra_description = wiki_summary\n",
    "    elif 'wikidata' in tags:\n",
    "        wikidata_id = tags['wikidata']\n",
    "        wiki_description = get_wikidata_description(wikidata_id)\n",
    "        if wiki_description:\n",
    "            extra_description = wiki_description\n",
    "\n",
    "    if extra_description:\n",
    "        description_parts.append(extra_description)\n",
    "\n",
    "    return \". \".join(description_parts)\n",
    "\n",
    "def process_city_pois(city_name):\n",
    "    print(f\"\\n--- Начинаем обработку города: {city_name} ---\")\n",
    "    overpass_query = f\"\"\"\n",
    "    [out:json][timeout:180];\n",
    "    area[name=\"{city_name}\"]->.searchArea;\n",
    "    (\n",
    "      node[\"tourism\"](area.searchArea);\n",
    "      way[\"tourism\"](area.searchArea);\n",
    "\n",
    "      node[\"leisure\"](area.searchArea);\n",
    "      way[\"leisure\"](area.searchArea);\n",
    "\n",
    "      node[\"historic\"](area.searchArea);\n",
    "      way[\"historic\"](area.searchArea);\n",
    "\n",
    "      node[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "      way[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "\n",
    "      node[\"natural\"~\"park|wood|garden|beach|peak|water\"](area.searchArea);\n",
    "      way[\"natural\"~\"park|wood|garden|beach|peak|water\"](area.searchArea);\n",
    "\n",
    "      node[\"amenity\"~\"arts_centre|theatre|cinema|museum|library|nightclub|bar|restaurant|cafe|pub|food_court|community_centre|marketplace|atm|bank|clinic|hospital|pharmacy|post_office|police|fire_station|school|university|kindergarten|dentist|veterinary|parking|toilets|fountain|place_of_worship|courthouse|embassy|townhall|public_bath|sauna|stripclub|brothel\"](area.searchArea);\n",
    "      way[\"amenity\"~\"arts_centre|theatre|cinema|museum|library|nightclub|bar|restaurant|cafe|pub|food_court|community_centre|marketplace|place_of_worship\"](area.searchArea);\n",
    "      node[\"amenity\"~\"^(arts_centre|theatre|cinema|museum|library|nightclub|bar|restaurant|cafe|pub|food_court|marketplace|fountain|public_bath|sauna|stripclub|brothel)$\"](area.searchArea);\n",
    "      way[\"amenity\"~\"^(arts_centre|theatre|cinema|museum|library|nightclub|bar|restaurant|cafe|pub|food_court|marketplace|fountain|public_bath|sauna|stripclub|brothel)$\"](area.searchArea);\n",
    "\n",
    "      node[\"shop\"~\"^(mall|department_store|books|gift|souvenir|art|antiques|craft|boutique|jewelry|leather|music|shoes|toys|video)$\"](area.searchArea);\n",
    "      way[\"shop\"~\"^(mall|department_store|books|gift|souvenir|art|antiques|craft|boutique|jewelry|leather|music|shoes|toys|video)$\"](area.searchArea);\n",
    "    );\n",
    "    out body;\n",
    "    >;\n",
    "    out skel qt;\n",
    "    \"\"\"\n",
    "\n",
    "    amenities_to_include = [\n",
    "        \"arts_centre\", \"theatre\", \"cinema\", \"museum\", \"library\", \"nightclub\", \"bar\",\n",
    "        \"restaurant\", \"cafe\", \"pub\", \"food_court\", \"marketplace\", \"fountain\",\n",
    "        \"public_bath\", \"sauna\", \"stripclub\", \"brothel\", \"casino\", \"ferry_terminal\",\n",
    "        \"attraction\", \"theme_park\", \"water_park\", \"zoo\", \"aquarium\", \"planetarium\",\n",
    "        \"gallery\", \"viewpoint\", \"observatory\"\n",
    "    ]\n",
    "    shops_to_include = [\n",
    "        \"mall\", \"department_store\", \"books\", \"gift\", \"souvenir\", \"art\", \"antiques\",\n",
    "        \"craft\", \"boutique\", \"jewelry\", \"leather\", \"music\", \"shoes\", \"toys\", \"video\",\n",
    "        \"kiosk\", \"convenience\"\n",
    "    ]\n",
    "\n",
    "    amenity_regex = \"|\".join(amenities_to_include)\n",
    "    shop_regex = \"|\".join(shops_to_include)\n",
    "\n",
    "    overpass_query_filtered = f\"\"\"\n",
    "    [out:json][timeout:180];\n",
    "    area[name=\"{city_name}\"]->.searchArea;\n",
    "    (\n",
    "      node[\"tourism\"](area.searchArea);\n",
    "      way[\"tourism\"](area.searchArea);\n",
    "      node[\"leisure\"](area.searchArea);\n",
    "      way[\"leisure\"](area.searchArea);\n",
    "      node[\"historic\"](area.searchArea);\n",
    "      way[\"historic\"](area.searchArea);\n",
    "\n",
    "      node[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "      way[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "      node[\"natural\"~\"park|wood|garden|beach|peak|water\"](area.searchArea);\n",
    "      way[\"natural\"~\"park|wood|garden|beach|peak|water\"](area.searchArea);\n",
    "\n",
    "      node[\"amenity\"~\"^{amenity_regex}$\"](area.searchArea);\n",
    "      way[\"amenity\"~\"^{amenity_regex}$\"](area.searchArea);\n",
    "\n",
    "      node[\"shop\"~\"^{shop_regex}$\"](area.searchArea);\n",
    "      way[\"shop\"~\"^{shop_regex}$\"](area.searchArea);\n",
    "    );\n",
    "    out body;\n",
    "    >;\n",
    "    out skel qt;\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"🚀 Отправляем запрос к Overpass API для {city_name}...\")\n",
    "    try:\n",
    "        response = requests.get(OVERPASS_URL, params={'data': overpass_query_filtered}, timeout=180)\n",
    "        print(f\"✅ Запрос для {city_name} выполнен с кодом: {response.status_code}\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "        else:\n",
    "            print(f\"Ошибка выполнения запроса для {city_name}: {response.text}\")\n",
    "            data = {'elements': []}\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Таймаут при запросе к Overpass API для {city_name}\")\n",
    "        data = {'elements': []}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Ошибка запроса к Overpass API для {city_name}: {e}\")\n",
    "        data = {'elements': []}\n",
    "\n",
    "\n",
    "    current_city_pois = []\n",
    "    print(f\"⚙️ Начинаем обработку {len(data['elements'])} объектов для {city_name}...\")\n",
    "\n",
    "    for element in tqdm(data['elements'], desc=f\"Обработка POI в {city_name}\"):\n",
    "        if 'tags' in element:\n",
    "            tags = element['tags']\n",
    "\n",
    "            if 'name' not in tags:\n",
    "                continue\n",
    "\n",
    "            lat, lon = (0, 0)\n",
    "            if element['type'] == 'node':\n",
    "                lat = element.get('lat')\n",
    "                lon = element.get('lon')\n",
    "            elif 'center' in element:\n",
    "                lat = element['center'].get('lat')\n",
    "                lon = element['center'].get('lon')\n",
    "\n",
    "            if lat == 0 and lon == 0:\n",
    "                continue\n",
    "\n",
    "            text_description = create_text_description(tags)\n",
    "\n",
    "            current_city_pois.append({\n",
    "                'id': element['id'],\n",
    "                'type': element['type'],\n",
    "                'lat': lat,\n",
    "                'lon': lon,\n",
    "                'name': tags.get('name'),\n",
    "                'city': city_name,\n",
    "                'text_description': text_description,\n",
    "                'tags': tags\n",
    "            })\n",
    "\n",
    "        time.sleep(0.005) # 5 миллисекунд\n",
    "\n",
    "    print(f\"👍 Обработано {len(current_city_pois)} релевантных объектов с названиями для {city_name}.\")\n",
    "\n",
    "    all_processed_pois.extend(current_city_pois)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cities_to_process = [\"Казань\", \"Санкт-Петербург\", \"Москва\"]\n",
    "\n",
    "    for city in cities_to_process:\n",
    "        process_city_pois(city)\n",
    "        time.sleep(10)\n",
    "\n",
    "    print(\"\\n--- Обработка всех городов завершена! ---\")\n",
    "    print(f\"Всего собрано {len(all_processed_pois)} POI.\")\n",
    "\n",
    "    if all_processed_pois:\n",
    "        final_df = pd.DataFrame(all_processed_pois)\n",
    "        final_df.drop_duplicates(subset=['id'], inplace=True)\n",
    "        print(f\"Всего уникальных POI после удаления дубликатов: {len(final_df)}\")\n",
    "\n",
    "        output_filename = 'poi_dataset_russia_filtered_enriched.csv'\n",
    "        final_df.to_csv(output_filename, index=False)\n",
    "        print(f\"💾 Окончательный датасет успешно сохранен в '{output_filename}'\")\n",
    "    else:\n",
    "        print(\"⚠️ Общий датасет пуст. Возможно, возникли ошибки при сборе данных.\")"
   ],
   "id": "ee5714ef775ca054",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Начинаем обработку города: Казань ---\n",
      "🚀 Отправляем запрос к Overpass API для Казань...\n",
      "✅ Запрос для Казань выполнен с кодом: 200\n",
      "⚙️ Начинаем обработку 73622 объектов для Казань...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка POI в Казань: 100%|██████████| 73622/73622 [06:29<00:00, 189.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👍 Обработано 3060 релевантных объектов с названиями для Казань.\n",
      "\n",
      "--- Начинаем обработку города: Санкт-Петербург ---\n",
      "🚀 Отправляем запрос к Overpass API для Санкт-Петербург...\n",
      "✅ Запрос для Санкт-Петербург выполнен с кодом: 200\n",
      "⚙️ Начинаем обработку 413806 объектов для Санкт-Петербург...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка POI в Санкт-Петербург: 100%|██████████| 413806/413806 [37:42<00:00, 182.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👍 Обработано 16858 релевантных объектов с названиями для Санкт-Петербург.\n",
      "\n",
      "--- Начинаем обработку города: Москва ---\n",
      "🚀 Отправляем запрос к Overpass API для Москва...\n",
      "✅ Запрос для Москва выполнен с кодом: 200\n",
      "⚙️ Начинаем обработку 608685 объектов для Москва...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка POI в Москва: 100%|██████████| 608685/608685 [54:07<00:00, 187.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👍 Обработано 21178 релевантных объектов с названиями для Москва.\n",
      "\n",
      "--- Обработка всех городов завершена! ---\n",
      "Всего собрано 41096 POI.\n",
      "Всего уникальных POI после удаления дубликатов: 41087\n",
      "💾 Окончательный датасет успешно сохранен в 'poi_dataset_russia_filtered_enriched.csv'\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T12:15:10.219522Z",
     "start_time": "2025-06-17T11:58:56.662281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "OVERPASS_URL = \"http://overpass-api.de/api/interpreter\"\n",
    "WIKIPEDIA_API_URL = \"https://ru.wikipedia.org/w/api.php\"\n",
    "WIKIDATA_API_URL = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "def get_wikipedia_summary(title):\n",
    "    try:\n",
    "        params = {\n",
    "            \"action\": \"query\", \"format\": \"json\", \"titles\": title,\n",
    "            \"prop\": \"extracts\", \"exintro\": True, \"explaintext\": True, \"redirects\": 1\n",
    "        }\n",
    "        response = requests.get(WIKIPEDIA_API_URL, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        page_id = next(iter(data['query']['pages']))\n",
    "        if page_id != '-1':\n",
    "            return data['query']['pages'][page_id].get('extract', '')\n",
    "        return \"\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"\"\n",
    "    except requests.exceptions.RequestException:\n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def get_wikidata_description(qid):\n",
    "    try:\n",
    "        params = {\n",
    "            \"action\": \"wbgetentities\", \"format\": \"json\", \"ids\": qid,\n",
    "            \"props\": \"descriptions\", \"languages\": \"ru|en\"\n",
    "        }\n",
    "        response = requests.get(WIKIDATA_API_URL, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        entity = data['entities'].get(qid)\n",
    "        if entity and 'descriptions' in entity:\n",
    "            if 'ru' in entity['descriptions']:\n",
    "                return entity['descriptions']['ru']['value']\n",
    "            elif 'en' in entity['descriptions']:\n",
    "                return entity['descriptions']['en']['value']\n",
    "        return \"\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"\"\n",
    "    except requests.exceptions.RequestException:\n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def create_text_description(tags):\n",
    "    description_parts = []\n",
    "    if 'name' in tags:\n",
    "        description_parts.append(tags['name'])\n",
    "\n",
    "    tag_translations = {\n",
    "        'amenity': 'Тип', 'shop': 'Магазин', 'tourism': 'Туризм',\n",
    "        'leisure': 'Досуг', 'historic': 'Исторический объект', 'cuisine': 'Кухня'\n",
    "    }\n",
    "    for key, name in tag_translations.items():\n",
    "        if key in tags:\n",
    "            description_parts.append(f\"{name}: {tags[key].replace('_', ' ')}\")\n",
    "\n",
    "    if 'opening_hours' in tags: description_parts.append(f\"Часы работы: {tags['opening_hours']}\")\n",
    "    if 'wheelchair' in tags and tags['wheelchair'] == 'yes': description_parts.append(\"Доступно для инвалидных колясок\")\n",
    "    if 'internet_access' in tags and tags['internet_access'] == 'yes': description_parts.append(\"Есть доступ в интернет\")\n",
    "    if 'phone' in tags: description_parts.append(f\"Телефон: {tags['phone']}\")\n",
    "    if 'website' in tags: description_parts.append(f\"Вебсайт: {tags['website']}\")\n",
    "    if 'description' in tags: description_parts.append(f\"Описание: {tags['description']}\")\n",
    "\n",
    "    extra_description = \"\"\n",
    "    if 'wikipedia' in tags:\n",
    "        parts = tags['wikipedia'].split(':')\n",
    "        if len(parts) >= 2:\n",
    "            lang = parts[0]\n",
    "            title = parts[-1]\n",
    "            if lang == 'ru':\n",
    "                wiki_summary = get_wikipedia_summary(title)\n",
    "                if wiki_summary: extra_description = wiki_summary\n",
    "    elif 'wikidata' in tags:\n",
    "        wikidata_id = tags['wikidata']\n",
    "        wiki_description = get_wikidata_description(wikidata_id)\n",
    "        if wiki_description: extra_description = wiki_description\n",
    "\n",
    "    if extra_description: description_parts.append(extra_description)\n",
    "    return \". \".join(description_parts)\n",
    "\n",
    "def process_city_pois(city_name):\n",
    "    print(f\"\\n--- Начинаем обработку города: {city_name} ---\")\n",
    "\n",
    "    amenities_to_include = [\n",
    "        \"arts_centre\", \"theatre\", \"cinema\", \"museum\", \"library\", \"nightclub\", \"bar\",\n",
    "        \"restaurant\", \"cafe\", \"pub\", \"food_court\", \"marketplace\", \"fountain\",\n",
    "        \"public_bath\", \"sauna\", \"casino\", \"ferry_terminal\", \"attraction\",\n",
    "        \"theme_park\", \"water_park\", \"zoo\", \"aquarium\", \"planetarium\", \"gallery\",\n",
    "        \"viewpoint\", \"observatory\", \"place_of_worship\", \"community_centre\", \"social_facility\"\n",
    "    ]\n",
    "    shops_to_include = [\n",
    "        \"mall\", \"department_store\", \"books\", \"gift\", \"souvenir\", \"art\", \"antiques\",\n",
    "        \"craft\", \"boutique\", \"jewelry\", \"leather\", \"music\", \"shoes\", \"toys\", \"video\",\n",
    "        \"convenience\", \"supermarket\", \"bakery\", \"beverages\", \"confectionery\", \"deli\",\n",
    "        \"farm\", \"greengrocer\", \"ice_cream\", \"pastry\", \"wine\", \"stationery\", \"sports\",\n",
    "        \"fashion\", \"perfumery\"\n",
    "    ]\n",
    "\n",
    "    amenity_filter = \"|\".join(amenities_to_include)\n",
    "    shop_filter = \"|\".join(shops_to_include)\n",
    "\n",
    "    overpass_query_strict = f\"\"\"\n",
    "    [out:json][timeout:180];\n",
    "    area[name=\"{city_name}\"]->.searchArea;\n",
    "    (\n",
    "      node[\"tourism\"](area.searchArea);\n",
    "      way[\"tourism\"](area.searchArea);\n",
    "      relation[\"tourism\"](area.searchArea);\n",
    "\n",
    "      node[\"leisure\"](area.searchArea);\n",
    "      way[\"leisure\"](area.searchArea);\n",
    "      relation[\"leisure\"](area.searchArea);\n",
    "\n",
    "      node[\"historic\"](area.searchArea);\n",
    "      way[\"historic\"](area.searchArea);\n",
    "      relation[\"historic\"](area.searchArea);\n",
    "\n",
    "      node[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "      way[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "\n",
    "      node[\"natural\"~\"park|wood|garden|beach|peak|water|forest|island|ridge|valley|volcano|wetland|glacier\"](area.searchArea);\n",
    "      way[\"natural\"~\"park|wood|garden|beach|peak|water|forest|island|ridge|valley|volcano|wetland|glacier\"](area.searchArea);\n",
    "      relation[\"natural\"~\"park|wood|garden|beach|peak|water|forest|island|ridge|valley|volcano|wetland|glacier\"](area.searchArea);\n",
    "\n",
    "      node[\"amenity\"~\"^{amenity_filter}$\"](area.searchArea);\n",
    "      way[\"amenity\"~\"^{amenity_filter}$\"](area.searchArea);\n",
    "\n",
    "      node[\"shop\"~\"^{shop_filter}$\"](area.searchArea);\n",
    "      way[\"shop\"~\"^{shop_filter}$\"](area.searchArea);\n",
    "\n",
    "      node[\"landuse\"~\"forest|park|recreation_ground|village_green\"](area.searchArea);\n",
    "      way[\"landuse\"~\"forest|park|recreation_ground|village_green\"](area.searchArea);\n",
    "    );\n",
    "    out body;\n",
    "    >;\n",
    "    out skel qt;\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"🚀 Отправляем фильтрованный запрос к Overpass API для {city_name}...\")\n",
    "    try:\n",
    "        response = requests.get(OVERPASS_URL, params={'data': overpass_query_strict}, timeout=180)\n",
    "        print(f\"✅ Запрос для {city_name} выполнен с кодом: {response.status_code}\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "        else:\n",
    "            print(f\"Ошибка выполнения запроса для {city_name}: {response.text}\")\n",
    "            data = {'elements': []}\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Таймаут при запросе к Overpass API для {city_name}. Попробуйте увеличить timeout.\")\n",
    "        data = {'elements': []}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Ошибка запроса к Overpass API для {city_name}: {e}\")\n",
    "        data = {'elements': []}\n",
    "\n",
    "    current_city_pois = []\n",
    "    print(f\"⚙️ Начинаем обработку {len(data['elements'])} объектов, полученных из Overpass для {city_name}...\")\n",
    "\n",
    "    for element in tqdm(data['elements'], desc=f\"Обработка и обогащение POI в {city_name}\"):\n",
    "        if 'tags' in element:\n",
    "            tags = element['tags']\n",
    "\n",
    "            if 'name' not in tags:\n",
    "                continue\n",
    "\n",
    "            lat, lon = (0, 0)\n",
    "            if element['type'] == 'node':\n",
    "                lat = element.get('lat')\n",
    "                lon = element.get('lon')\n",
    "            elif element['type'] == 'way' and 'center' in element:\n",
    "                lat = element['center'].get('lat')\n",
    "                lon = element['center'].get('lon')\n",
    "            elif element['type'] == 'relation' and 'center' in element:\n",
    "                lat = element['center'].get('lat')\n",
    "                lon = element['center'].get('lon')\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if lat == 0 and lon == 0:\n",
    "                continue\n",
    "\n",
    "            text_description = create_text_description(tags)\n",
    "\n",
    "            current_city_pois.append({\n",
    "                'id': element['id'],\n",
    "                'type': element['type'],\n",
    "                'lat': lat,\n",
    "                'lon': lon,\n",
    "                'name': tags.get('name'),\n",
    "                'city': city_name,\n",
    "                'text_description': text_description,\n",
    "                'tags': tags\n",
    "            })\n",
    "\n",
    "        time.sleep(0.005)\n",
    "\n",
    "    print(f\"👍 Всего обработано и добавлено в список: {len(current_city_pois)} POI для {city_name}.\")\n",
    "\n",
    "    return current_city_pois\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_filename = 'poi_dataset_russia_filtered_enriched.csv'\n",
    "\n",
    "    # 1. Загружаем существующий датасет, если он есть\n",
    "    if os.path.exists(output_filename):\n",
    "        print(f\"Загружаем существующий датасет из '{output_filename}'...\")\n",
    "        existing_df = pd.read_csv(output_filename)\n",
    "        print(f\"Загружено {len(existing_df)} POI из существующего файла.\")\n",
    "    else:\n",
    "        print(f\"Файл '{output_filename}' не найден. Создадим новый датасет.\")\n",
    "        existing_df = pd.DataFrame()\n",
    "\n",
    "    cities_to_add = [\"Екатеринбург\"]\n",
    "\n",
    "    newly_collected_pois = []\n",
    "    for city in cities_to_add:\n",
    "        city_data = process_city_pois(city)\n",
    "        newly_collected_pois.extend(city_data)\n",
    "        time.sleep(10)\n",
    "\n",
    "    print(\"\\n--- Сбор данных для новых городов завершен! ---\")\n",
    "    print(f\"Всего собрано {len(newly_collected_pois)} новых POI.\")\n",
    "\n",
    "    combined_pois = pd.concat([existing_df, pd.DataFrame(newly_collected_pois)], ignore_index=True)\n",
    "\n",
    "    print(f\"Всего POI до удаления дубликатов: {len(combined_pois)}\")\n",
    "    combined_pois.dropna(subset=['name', 'lat', 'lon'], inplace=True)\n",
    "    combined_pois.drop_duplicates(subset=['id'], inplace=True)\n",
    "\n",
    "    print(f\"Итоговое количество уникальных и полных POI: {len(combined_pois)}\")\n",
    "\n",
    "    combined_pois.to_csv(output_filename, index=False)\n",
    "    print(f\"💾 Обновленный датасет успешно сохранен в '{output_filename}'\")"
   ],
   "id": "b39d2a53a961737b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем существующий датасет из 'poi_dataset_russia_filtered_enriched.csv'...\n",
      "Загружено 46354 POI из существующего файла.\n",
      "\n",
      "--- Начинаем обработку города: Екатеринбург ---\n",
      "🚀 Отправляем фильтрованный запрос к Overpass API для Екатеринбург...\n",
      "✅ Запрос для Екатеринбург выполнен с кодом: 200\n",
      "⚙️ Начинаем обработку 173809 объектов, полученных из Overpass для Екатеринбург...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка и обогащение POI в Екатеринбург: 100%|██████████| 173809/173809 [15:47<00:00, 183.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👍 Всего обработано и добавлено в список: 4109 POI для Екатеринбург.\n",
      "\n",
      "--- Сбор данных для новых городов завершен! ---\n",
      "Всего собрано 4109 новых POI.\n",
      "Всего POI до удаления дубликатов: 50463\n",
      "Итоговое количество уникальных и полных POI: 50463\n",
      "💾 Обновленный датасет успешно сохранен в 'poi_dataset_russia_filtered_enriched.csv'\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
